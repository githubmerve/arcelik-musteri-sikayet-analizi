{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0db4e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def start_stable_driver():\n",
    "    options = Options()\n",
    "    options.page_load_strategy = 'eager'\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_experimental_option(\"detach\", True)\n",
    "    options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    \n",
    "    # HÄ±z ve stabilite iÃ§in resimleri yÃ¼kleme\n",
    "    prefs = {\"profile.managed_default_content_settings.images\": 2}\n",
    "    options.add_experimental_option(\"prefs\", prefs)\n",
    "    \n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "    return driver\n",
    "\n",
    "# Global DeÄŸiÅŸkenler\n",
    "BASE_URL = \"https://www.sikayetvar.com/arcelik/kurutma-makinesi\"\n",
    "dryer_links = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ac0d7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Kurutma Makinesi linkleri toplanÄ±yor...\n",
      "âœ… Sayfa 10 bitti. Havuz: 231 link.\n",
      "âœ… Sayfa 20 bitti. Havuz: 460 link.\n",
      "ğŸ§¹ Bellek temizlendi, devam ediliyor...\n",
      "âœ… Sayfa 30 bitti. Havuz: 635 link.\n",
      "âœ… Sayfa 40 bitti. Havuz: 744 link.\n",
      "ğŸ§¹ Bellek temizlendi, devam ediliyor...\n",
      "âœ… Sayfa 50 bitti. Havuz: 832 link.\n",
      "âœ… Sayfa 60 bitti. Havuz: 900 link.\n",
      "ğŸ§¹ Bellek temizlendi, devam ediliyor...\n",
      "âœ… Sayfa 70 bitti. Havuz: 960 link.\n",
      "âœ… Sayfa 80 bitti. Havuz: 1019 link.\n",
      "ğŸ§¹ Bellek temizlendi, devam ediliyor...\n",
      "âœ… Sayfa 90 bitti. Havuz: 1055 link.\n",
      "âœ… Sayfa 100 bitti. Havuz: 1098 link.\n",
      "ğŸ§¹ Bellek temizlendi, devam ediliyor...\n",
      "âœ… Sayfa 110 bitti. Havuz: 1133 link.\n",
      "âœ… Sayfa 120 bitti. Havuz: 1167 link.\n",
      "ğŸ§¹ Bellek temizlendi, devam ediliyor...\n",
      "âœ… Sayfa 130 bitti. Havuz: 1201 link.\n",
      "ğŸ§¹ Bellek temizlendi, devam ediliyor...\n",
      "ğŸ Link Toplama Bitti! Toplam 1225 link hazÄ±r.\n"
     ]
    }
   ],
   "source": [
    "target_pages = 137 \n",
    "current_page = 1 \n",
    "refresh_limit = 20 \n",
    "\n",
    "print(f\"ğŸš€ Kurutma Makinesi linkleri toplanÄ±yor...\")\n",
    "\n",
    "while current_page <= target_pages:\n",
    "    driver = start_stable_driver()\n",
    "    next_stop = min(current_page + refresh_limit - 1, target_pages)\n",
    "    \n",
    "    try:\n",
    "        for p in range(current_page, next_stop + 1):\n",
    "            success = False\n",
    "            attempts = 0\n",
    "            while not success and attempts < 2:\n",
    "                try:\n",
    "                    driver.get(f\"{BASE_URL}?page={p}\")\n",
    "                    time.sleep(2.5)\n",
    "                    \n",
    "                    links = driver.find_elements(By.CSS_SELECTOR, \"a.complaint-layer\")\n",
    "                    if len(links) > 0:\n",
    "                        for l in links:\n",
    "                            href = l.get_attribute(\"href\")\n",
    "                            title = l.get_attribute(\"title\")\n",
    "                            if href and \"/arcelik/\" in href:\n",
    "                                if not any(d['url'] == href for d in dryer_links):\n",
    "                                    dryer_links.append({\"baslik\": title, \"url\": href})\n",
    "                        success = True\n",
    "                    else:\n",
    "                        time.sleep(5) # Captcha kontrolÃ¼\n",
    "                        attempts += 1\n",
    "                except:\n",
    "                    attempts += 1\n",
    "            \n",
    "            if p % 10 == 0:\n",
    "                print(f\"âœ… Sayfa {p} bitti. Havuz: {len(dryer_links)} link.\")\n",
    "                \n",
    "            current_page = p + 1\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    driver.quit()\n",
    "    print(\"ğŸ§¹ Bellek temizlendi, devam ediliyor...\")\n",
    "\n",
    "print(f\"ğŸ Link Toplama Bitti! Toplam {len(dryer_links)} link hazÄ±r.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa780ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ 1225 adet kurutma makinesi ÅŸikayeti Ã§ekiliyor...\n",
      "ğŸ“Š Ä°lerleme: 100/1225 | Kalan: ~13.5 dk\n",
      "ğŸ“Š Ä°lerleme: 200/1225 | Kalan: ~12.4 dk\n",
      "ğŸ“Š Ä°lerleme: 300/1225 | Kalan: ~11.5 dk\n",
      "ğŸ“Š Ä°lerleme: 400/1225 | Kalan: ~10.1 dk\n",
      "ğŸ“Š Ä°lerleme: 500/1225 | Kalan: ~8.5 dk\n",
      "ğŸ“Š Ä°lerleme: 600/1225 | Kalan: ~7.1 dk\n",
      "ğŸ“Š Ä°lerleme: 700/1225 | Kalan: ~6.2 dk\n",
      "ğŸ“Š Ä°lerleme: 800/1225 | Kalan: ~5.2 dk\n",
      "ğŸ“Š Ä°lerleme: 900/1225 | Kalan: ~3.8 dk\n",
      "ğŸ“Š Ä°lerleme: 1000/1225 | Kalan: ~2.6 dk\n",
      "ğŸ“Š Ä°lerleme: 1100/1225 | Kalan: ~1.5 dk\n",
      "ğŸ“Š Ä°lerleme: 1200/1225 | Kalan: ~0.3 dk\n",
      "âœ… Ä°ÅLEM TAMAMLANDI!\n"
     ]
    }
   ],
   "source": [
    "session = requests.Session()\n",
    "adapter = requests.adapters.HTTPAdapter(pool_connections=50, pool_maxsize=50)\n",
    "session.mount('https://', adapter)\n",
    "\n",
    "def fetch_dryer_ultra_compatible(item):\n",
    "    url = item['url']\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\"}\n",
    "    \n",
    "    for attempt in range(5): # Deneme sayÄ±sÄ±nÄ± 5'e Ã§Ä±kardÄ±k\n",
    "        try:\n",
    "            time.sleep(random.uniform(0.5, 1.0))\n",
    "            res = session.get(url, headers=headers, timeout=15)\n",
    "            \n",
    "            if res.status_code == 200:\n",
    "                soup = BeautifulSoup(res.text, 'html.parser')\n",
    "                content_parts = []\n",
    "                \n",
    "                # YÃ–NTEM 1: Modern TasarÄ±m (data-ga etiketleri)\n",
    "                start_node = soup.find(\"div\", {\"data-ga-element\": \"Complaint_Content_Start\"})\n",
    "                if start_node:\n",
    "                    for sibling in start_node.find_next_siblings():\n",
    "                        if sibling.name == \"div\" and sibling.get(\"data-ga-element\") == \"Complaint_Content_End\": break\n",
    "                        if sibling.name == \"p\": content_parts.append(sibling.get_text(strip=True))\n",
    "                \n",
    "                # YÃ–NTEM 2: Orta Nesil TasarÄ±m (description divi)\n",
    "                if not content_parts:\n",
    "                    desc_div = soup.find(\"div\", class_=\"complaint-detail-description\")\n",
    "                    if desc_div: content_parts = [desc_div.get_text(strip=True)]\n",
    "                \n",
    "                # YÃ–NTEM 3: ArÅŸiv/Eski TasarÄ±m (yalÄ±n p etiketleri)\n",
    "                if not content_parts:\n",
    "                    p_tags = soup.select(\".description p\") or soup.select(\".complaint-detail-description p\")\n",
    "                    content_parts = [p.get_text(strip=True) for p in p_tags]\n",
    "\n",
    "                full_text = \" \".join(content_parts)\n",
    "                if len(full_text) > 10:\n",
    "                    return {\"baslik\": item[\"baslik\"], \"url\": url, \"aciklama\": full_text}\n",
    "            \n",
    "            elif res.status_code == 429:\n",
    "                time.sleep(15) # HÄ±z sÄ±nÄ±rÄ±na takÄ±lÄ±rsa uzun mola\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    return {\"baslik\": item[\"baslik\"], \"url\": url, \"aciklama\": \"HATA: AlÄ±namadÄ±.\"}\n",
    "\n",
    "# --- Ã‡ALIÅTIRICI ---\n",
    "print(f\"ğŸ“¥ {len(dryer_links)} adet kurutma makinesi ÅŸikayeti Ã§ekiliyor...\")\n",
    "final_dryer_results = []\n",
    "start_time = time.time()\n",
    "\n",
    "# Sunucuyu yormamak iÃ§in max_workers=6 idealdir\n",
    "with ThreadPoolExecutor(max_workers=6) as executor:\n",
    "    futures = [executor.submit(fetch_dryer_ultra_compatible, link) for link in dryer_links]\n",
    "    for i, future in enumerate(futures, 1):\n",
    "        final_dryer_results.append(future.result())\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            rem = (elapsed / i) * (len(dryer_links) - i) / 60\n",
    "            print(f\"ğŸ“Š Ä°lerleme: {i}/{len(dryer_links)} | Kalan: ~{rem:.1f} dk\")\n",
    "            # Her 200'de bir yedek al\n",
    "            pd.DataFrame(final_dryer_results).to_excel(\"arcelik_kurutma_GUNCEL_YEDEK.xlsx\", index=False)\n",
    "\n",
    "# Final KayÄ±t\n",
    "pd.DataFrame(final_dryer_results).to_excel(\"arcelik_kurutma_makinesi_YENI_FINAL.xlsx\", index=False)\n",
    "print(\"âœ… Ä°ÅLEM TAMAMLANDI!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
